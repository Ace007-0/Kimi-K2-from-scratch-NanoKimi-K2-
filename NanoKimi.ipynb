{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8dd904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import os\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67494d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration & Setup\n",
    "# =======================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# =======================\n",
    "# Load 10% of TinyStories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10%]\")\n",
    "texts = [item['text'] for item in dataset]\n",
    "\n",
    "# Save texts to file for tokenizer training\n",
    "with open(\"corpus.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac282c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SentencePiece Tokenizer\n",
    "# =======================\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='corpus.txt', model_prefix='tokenizer', vocab_size=4096, model_type='bpe'\n",
    ")\n",
    "sp = spm.SentencePieceProcessor(model_file='tokenizer.model')\n",
    "vocab_size = sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Dataset\n",
    "# =======================\n",
    "tokens = []\n",
    "for text in texts:\n",
    "    tokens.extend(sp.encode(text))\n",
    "data = torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation Split\n",
    "# =======================\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e663934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Preparation\n",
    "# =======================\n",
    "block_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    src = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(src) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([src[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([src[i+1:i+1+block_size] for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "# =======================\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=-1)\n",
    "        return F.silu(x1) * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert and MoE Layer\n",
    "# =======================\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 2 * dim),\n",
    "            SwiGLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, dim, num_experts=4):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([Expert(dim) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        weights = F.softmax(self.gate(x), dim=-1)\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=0)\n",
    "        weights = weights.permute(2, 0, 1).unsqueeze(-1)\n",
    "        out = (weights * expert_outputs).sum(dim=0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Attention Layer\n",
    "# =======================\n",
    "class LatentAttention(nn.Module):\n",
    "    def __init__(self, dim, latent_dim=64, num_latents=16):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(1, num_latents, latent_dim))\n",
    "        self.to_q = nn.Linear(latent_dim, latent_dim)\n",
    "        self.to_kv = nn.Linear(dim, 2 * latent_dim)\n",
    "        self.to_out = nn.Linear(latent_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        latents = self.latents.expand(B, -1, -1)\n",
    "        q = self.to_q(latents)\n",
    "        k, v = self.to_kv(x).chunk(2, dim=-1)\n",
    "        attn = (q @ k.transpose(-2, -1)) / (k.shape[-1] ** 0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = attn @ v\n",
    "        out = self.to_out(out).mean(dim=1, keepdim=True)\n",
    "        return x + out.expand(-1, T, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "# =======================\n",
    "class NanoKimiTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, dim=256, depth=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(dim),\n",
    "                LatentAttention(dim),\n",
    "                nn.LayerNorm(dim),\n",
    "                MoELayer(dim)\n",
    "            ) for _ in range(depth)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.token_emb(x) + self.pos_emb(torch.arange(T, device=x.device))\n",
    "        for block in self.blocks:\n",
    "            x = x + block(x)\n",
    "        return self.head(self.ln(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muon Optimizer\n",
    "# =======================\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), weight_decay=1e-2):\n",
    "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            beta1, beta2 = group[\"betas\"]\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                g = p.grad\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p)\n",
    "                    state[\"exp_avg_diff\"] = torch.zeros_like(p)\n",
    "\n",
    "                exp_avg, exp_avg_diff = state[\"exp_avg\"], state[\"exp_avg_diff\"]\n",
    "                update = p - exp_avg_diff\n",
    "                exp_avg.mul_(beta1).add_(g, alpha=1 - beta1)\n",
    "                exp_avg_diff.mul_(beta2).add_(update, alpha=1 - beta2)\n",
    "\n",
    "                if weight_decay > 0:\n",
    "                    p.data.mul_(1 - lr * weight_decay)\n",
    "\n",
    "                p.add_(exp_avg_diff, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Utilities\n",
    "# =======================\n",
    "model = NanoKimiTransformer(vocab_size).to(device)\n",
    "optimizer = Muon(model.parameters(), lr=1e-3, betas=(0.9, 0.99), weight_decay=1e-2)\n",
    "\n",
    "def compute_val_loss(model, val_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = get_batch('val')\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "    model.train()\n",
    "    return loss.item()\n",
    "\n",
    "def train(model, optimizer, data, steps=5000):\n",
    "    model.train()\n",
    "    for step in range(steps):\n",
    "        x, y = get_batch('train')\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            val_loss = compute_val_loss(model, val_data)\n",
    "            print(f\"Step {step}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c19feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "# =======================\n",
    "train(model, optimizer, train_data, steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292af44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation\n",
    "# =======================\n",
    "def generate_text(prompt, steps=50, temperature=0.8, top_k=50):\n",
    "    model.eval()\n",
    "    tokens = torch.tensor([sp.encode(prompt)], dtype=torch.long).to(device)\n",
    "    for _ in range(steps):\n",
    "        logits = model(tokens)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "        probs = F.softmax(top_k_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "        next_token = top_k_indices.gather(-1, next_token)\n",
    "        tokens = torch.cat([tokens, next_token], dim=1)\n",
    "    return sp.decode(tokens[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0672816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Prompt Box\n",
    "# =======================\n",
    "def on_generate_click(b):\n",
    "    prompt = prompt_box.value\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"Generated:\", generate_text(prompt, steps=50, temperature=0.8, top_k=50))\n",
    "\n",
    "prompt_box = widgets.Text(value=\"Once upon a time\", description=\"Prompt:\")\n",
    "generate_button = widgets.Button(description=\"Generate\")\n",
    "generate_button.on_click(on_generate_click)\n",
    "display(prompt_box, generate_button)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
